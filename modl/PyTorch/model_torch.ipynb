{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code will create the model described in our following paper\n",
    "MoDL: Model-Based Deep Learning Architecture for Inverse Problems\n",
    "by H.K. Aggarwal, M.P. Mani, M. Jacob from University of Iowa.\n",
    "\n",
    "Paper dwonload  Link:     https://arxiv.org/abs/1712.02862\n",
    "\n",
    "@author: haggarwal\n",
    "\"\"\"\n",
    "\n",
    "'#PyTorch translation of the modelV2.py'  \n",
    "import torch\n",
    "import numpy as np\n",
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "epsilon=1e-5\n",
    "TFeps=torch.tensor(1e-5,dtype=torch.float32)\n",
    "\n",
    "# function c2r contatenate complex input as new axis two two real inputs\n",
    "c2r=lambda x:torch.stack([torch.real(x),torch.imag(x)],dim=-1)\n",
    "#r2c takes the last dimension of real input and converts to complex\n",
    "r2c=lambda x:torch.complex(x[...,:1],x[...,1:])\n",
    "\n",
    "def createLayer(x, szW, trainning,lastLayer):\n",
    "    \"\"\"\n",
    "    This function create a layer of CNN consisting of convolution, batch-norm,\n",
    "    and ReLU. Last layer does not have ReLU to avoid truncating the negative\n",
    "    part of the learned noise and alias patterns.\n",
    "    \"\"\"\n",
    "    W=torch.nn.Parameter(torch.randn(szW,dtype=torch.float32)*0.01)\n",
    "    x = torch.nn.functional.conv2d(input=x, weight=W, stride=[1, 1, 1, 1], padding='SAME')\n",
    "    xbn=torch.nn.BatchNorm2d(x.shape[1],affine=True,track_running_stats=True)\n",
    "    xbn.train(trainning)\n",
    "    xbn=xbn(x)\n",
    "    if not(lastLayer):\n",
    "        return torch.nn.functional.relu(xbn)\n",
    "    else:\n",
    "        return xbn\n",
    "\n",
    "def dw(inp,trainning,nLay):\n",
    "    \"\"\"\n",
    "    This is the Dw block as defined in the Fig. 1 of the MoDL paper\n",
    "    It creates an n-layer (nLay) residual learning CNN.\n",
    "    Convolution filters are of size 3x3 and 64 such filters are there.\n",
    "    nw: It is the learned noise\n",
    "    dw: it is the output of residual learning after adding the input back.\n",
    "    \"\"\"\n",
    "    lastLayer=False\n",
    "    nw={}\n",
    "    nw['c'+str(0)]=inp\n",
    "    szW={}\n",
    "    szW = {key: (3,3,64,64) for key in range(2,nLay)}\n",
    "    szW[1]=(3,3,2,64)\n",
    "    szW[nLay]=(3,3,64,2)\n",
    "\n",
    "    for i in np.arange(1,nLay+1):\n",
    "        if i==nLay:\n",
    "            lastLayer=True\n",
    "        with torch.no_grad():\n",
    "            nw['c'+str(i)]=createLayer(nw['c'+str(i-1)],szW[i],trainning,lastLayer)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        shortcut=torch.identity(inp)\n",
    "        dw=shortcut+nw['c'+str(nLay)]\n",
    "    return dw\n",
    "\n",
    "'#PyTorch translation of the code below'\n",
    "'# Translate class Aclass from modelV2.py to torch'\n",
    "\n",
    "class Aclass:\n",
    "    \"\"\"\n",
    "    This class is created to do the data-consistency (DC) step as described in paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, csm,mask,lam):\n",
    "        self.nrow,self.ncol=mask.shape[0],mask.shape[1]\n",
    "        self.pixels=self.nrow*self.ncol\n",
    "        self.mask=mask\n",
    "        self.csm=csm\n",
    "        self.SF=np.sqrt(self.pixels)\n",
    "        self.lam=lam\n",
    "        #self.cgIter=cgIter\n",
    "        #self.tol=tol\n",
    "    def myAtA(self,img):\n",
    "        with torch.no_grad():\n",
    "            coilImages=self.csm*img\n",
    "            kspace=  torch.fft(coilImages)/self.SF\n",
    "            temp=kspace*self.mask\n",
    "            coilImgs =torch.ifft(temp)*self.SF\n",
    "            coilComb= torch.sum(coilImgs*torch.conj(self.csm),dim=0)\n",
    "            coilComb=coilComb+self.lam*img\n",
    "        return coilComb\n",
    "\n",
    "def myCG(A,rhs):\n",
    "    \"\"\"\n",
    "    This is my implementation of CG algorithm in tensorflow that works on\n",
    "    complex data and runs on GPU. It takes the class object as input.\n",
    "    \"\"\"\n",
    "    rhs=r2c(rhs)\n",
    "    cond=lambda i,rTr,*_: torch.logical_and( torch.less(i,10), rTr>1e-10)\n",
    "    def body(i,rTr,x,r,p):\n",
    "        with torch.no_grad():\n",
    "            Ap=A.myAtA(p)\n",
    "            alpha = rTr / torch.cast(torch.reduce_sum(torch.conj(p)*Ap), dtype=torch.float32)\n",
    "            alpha=torch.complex(alpha,0.)\n",
    "            x = x + alpha * p\n",
    "            r = r - alpha * Ap\n",
    "            rTrNew = torch.cast( torch.reduce_sum(torch.conj(r)*r), dtype=torch.float32)\n",
    "            beta = rTrNew / rTr\n",
    "            beta=torch.complex(beta,0.)\n",
    "            p = r + beta * p\n",
    "        return i+1,rTrNew,x,r,p\n",
    "\n",
    "    x=torch.zeros_like(rhs)\n",
    "    i,r,p=0,rhs,rhs\n",
    "    rTr = torch.cast( torch.reduce_sum(torch.conj(r)*r), dtype=torch.float32,)\n",
    "    loopVar=i,rTr,x,r,p\n",
    "    out=torch.while_loop(cond=cond,body=body,loop_vars=loopVar,name='CGwhile',parallel_iterations=1)[2]\n",
    "    return c2r(out)\n",
    "\n",
    "def getLambda():\n",
    "    \"\"\"\n",
    "    create a shared variable called lambda\n",
    "    \"\"\"\n",
    "    lam = torch.nn.Parameter(torch.tensor(0.1,dtype=torch.float32))\n",
    "    return lam\n",
    "\n",
    "'# Translate function call CG from modelV2.py to torch'\n",
    "def callCG(rhs):\n",
    "    \"\"\"\n",
    "    this function will call the function myCG on each image in a batch\n",
    "    \"\"\"\n",
    "    G=torch.get_default_graph()\n",
    "    getnext=G.get_operation_by_name('getNext')\n",
    "    _,_,csm,mask=getnext.outputs\n",
    "    l=getLambda()\n",
    "    l2=torch.complex(l,0.)\n",
    "    def fn(tmp):\n",
    "        c,m,r=tmp\n",
    "        Aobj=Aclass(c,m,l2)\n",
    "        y=myCG(Aobj,r)\n",
    "        return y\n",
    "    inp=(csm,mask,rhs)\n",
    "    rec=torch.map_fn(fn,inp,dtype=torch.float32,name='mapFn2' )\n",
    "    return rec\n",
    "\n",
    "\n",
    "'# Translate decorator @tf.custom_gradient and function dcManualGradient from modelV2.py to torch'\n",
    "class dcManualGradient(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    This function impose data consistency constraint. Rather than relying on\n",
    "    TensorFlow to calculate the gradient for the conjuagte gradient part.\n",
    "    We can calculate the gradient manually as well by using this function.\n",
    "    Please see section III (c) in the paper.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        y=callCG(x)\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        out=callCG(grad_output)\n",
    "        return out\n",
    "\n",
    "def dc(rhs,csm,mask,lam1):\n",
    "    \"\"\"\n",
    "    This function is called to create testing model. It apply CG on each image\n",
    "    in the batch.\n",
    "    \"\"\"\n",
    "    lam2=torch.complex(lam1,0.)\n",
    "    def fn( tmp ):\n",
    "        c,m,r=tmp\n",
    "        Aobj=Aclass( c,m,lam2 )\n",
    "        y=myCG(Aobj,r)\n",
    "        return y\n",
    "    inp=(csm,mask,rhs)\n",
    "    rec=torch.map_fn(fn,inp,dtype=torch.float32,name='mapFn' )\n",
    "    return rec\n",
    "\n",
    "'# Translate function makeModel from modelV2.py to torch'\n",
    "def makeModel(atb,csm,mask,training,nLayers,K,gradientMethod):\n",
    "    \"\"\"\n",
    "    This is the main function that creates the model.\n",
    "\n",
    "    \"\"\"\n",
    "    out={}\n",
    "    out['dc0']=atb\n",
    "    with torch.name_scope('myModel'):\n",
    "        with torch.variable_scope('Wts',reuse=tf.AUTO_REUSE):\n",
    "            for i in range(1,K+1):\n",
    "                j=str(i)\n",
    "                out['dw'+j]=dw(out['dc'+str(i-1)],training,nLayers)\n",
    "                lam1=getLambda()\n",
    "                rhs=atb + lam1*out['dw'+j]\n",
    "                if gradientMethod=='AG':\n",
    "                    out['dc'+j]=dc(rhs,csm,mask,lam1)\n",
    "                elif gradientMethod=='MG':\n",
    "                    if training:\n",
    "                        out['dc'+j]=dcManualGradient(rhs)\n",
    "                    else:\n",
    "                        out['dc'+j]=dc(rhs,csm,mask,lam1)\n",
    "    return out\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
