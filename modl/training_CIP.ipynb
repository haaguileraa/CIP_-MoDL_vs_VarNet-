{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py as h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "\n",
    "\n",
    "# modelDir='knee_trained_MoDL'\n",
    "\n",
    "# loadChkPoint=tf.train.latest_checkpoint(modelDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['ismrmrd_header', 'kspace', 'reconstruction_rss']>\n",
      "<HDF5 dataset \"reconstruction_rss\": shape (12, 320, 320), type \"<f4\">\n",
      "<HDF5 dataset \"kspace\": shape (12, 20, 640, 320), type \"<c8\">\n",
      "<KeysViewHDF5 ['atb', 'org']>\n"
     ]
    }
   ],
   "source": [
    "with h5.File('/home/hpc/iwbi/iwbi009h/CIP_-MoDL_vs_VarNet-/test/file_brain_AXT1_201_6002695.h5','r') as f:\n",
    "    print(f.keys())\n",
    "    print(f['reconstruction_rss'])\n",
    "    print(f['kspace'])\n",
    "    with h5.File('test.h5', 'w') as target:\n",
    "        target.create_dataset('atb', data = f['reconstruction_rss'])\n",
    "        target.create_dataset('org', data = f['kspace'])\n",
    "        print(target.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['atb', 'csm', 'mask', 'org']>\n",
      "<HDF5 dataset \"atb\": shape (1, 640, 368), type \"<c8\">\n",
      "<HDF5 dataset \"org\": shape (1, 640, 368), type \"<c8\">\n",
      "<HDF5 dataset \"mask\": shape (1, 640, 368), type \"<c8\">\n"
     ]
    }
   ],
   "source": [
    "with h5.File('knee_demo_data.h5','r') as f:\n",
    "    print(f.keys())\n",
    "    print(f['atb'])\n",
    "    print(f['org'])\n",
    "    print(f['mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['tstCsm', 'tstMask', 'tstOrg']>\n",
      "<HDF5 dataset \"tstMask\": shape (1, 256, 232), type \"|i1\">\n",
      "<HDF5 dataset \"tstCsm\": shape (1, 12, 256, 232), type \"<c8\">\n",
      "<HDF5 dataset \"tstOrg\": shape (1, 256, 232), type \"<c8\">\n"
     ]
    }
   ],
   "source": [
    "with h5.File('demoImage.hdf5','r') as f:\n",
    "    print(f.keys())\n",
    "    print(f['tstMask'])\n",
    "    print(f['tstCsm'])\n",
    "    print(f['tstOrg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the original  MODL's dataset: \n",
    "# <KeysViewHDF5 ['trnCsm', 'trnMask', 'trnOrg', 'tstCsm', 'tstMask', 'tstOrg']>\n",
    "\n",
    "# ... \n",
    "# <HDF5 dataset \"trnCsm\": shape (360, 12, 256, 232), type \"<c8\">\n",
    "# <HDF5 dataset \"trnMask\": shape (360, 256, 232), type \"|i1\">\n",
    "# <HDF5 dataset \"trnOrg\": shape (360, 256, 232), type \"<c8\">\n",
    "# <HDF5 dataset \"tstOrg\": shape (164, 256, 232), type \"<c8\">\n",
    "# <HDF5 dataset \"tstMask\": shape (164, 256, 232), type \"|i1\">\n",
    "# >>> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************************\n",
      "WARNING:tensorflow:From /home/woody/iwbi/iwbi009h/software/privat/conda/envs/myenv/lib/python3.10/site-packages/tensorflow/python/util/deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hpc/iwbi/iwbi009h/CIP_-MoDL_vs_VarNet-/modl/modelV2.py:31: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  xbn=tf.compat.v1.layers.batch_normalization(x,training=trainning,fused=True,name='BN')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/woody/iwbi/iwbi009h/software/privat/conda/envs/myenv/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "# import some libraries\n",
    "import os,time\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import supportingFunctions as sf\n",
    "import modelV2 as mm\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "#% SET THESE PARAMETERS CAREFULLY\n",
    "nLayers=5\n",
    "epochs=50\n",
    "batchSize=1\n",
    "gradientMethod='AG'\n",
    "K=1\n",
    "sigma=0.01\n",
    "restoreWeights=False\n",
    "#%% to train the model with higher K values  (K>1) such as K=5 or 10,\n",
    "# it is better to initialize with a pre-trained model with K=1.\n",
    "if K>1:\n",
    "    restoreWeights=True\n",
    "    restoreFromModel='04Jun_0243pm_5L_1K_100E_AG'\n",
    "\n",
    "if restoreWeights:\n",
    "    wts=sf.getWeights('savedModels/'+restoreFromModel)\n",
    "#--------------------------------------------------------------------------\n",
    "#%%Generate a meaningful filename to save the trainined models for testing\n",
    "print ('*************************************************')\n",
    "start_time=time.time()\n",
    "saveDir='savedModels/'\n",
    "cwd=os.getcwd()\n",
    "directory=saveDir+datetime.now().strftime(\"%d%b_%I%M%P_\")+ \\\n",
    " str(nLayers)+'L_'+str(K)+'K_'+str(epochs)+'E_'+gradientMethod\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "sessFileName= directory+'/model'\n",
    "\n",
    "\n",
    "#%% save test model\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "csmT = tf.compat.v1.placeholder(tf.complex64,shape=(None,12,256,232),name='csm')\n",
    "maskT= tf.compat.v1.placeholder(tf.complex64,shape=(None,256,232),name='mask')\n",
    "atbT = tf.compat.v1.placeholder(tf.float32,shape=(None,256,232,2),name='atb')\n",
    "\n",
    "out=mm.makeModel(atbT,csmT,maskT,False,nLayers,K,gradientMethod)\n",
    "predTst=out['dc'+str(K)]\n",
    "predTst=tf.identity(predTst,name='predTst')\n",
    "sessFileNameTst=directory+'/modelTst'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing model saved:savedModels/10Feb_1223pm_5L_1K_50E_AG/modelTst\n",
      "Reading the data. Please wait...\n",
      "Elapsed time: 35.597615 seconds.\n",
      "\n",
      "Successfully read the data from file!\n",
      "Now doing undersampling....\n",
      "Elapsed time: 111.887410 seconds.\n",
      "\n",
      "Successfully undersampled data!\n",
      "WARNING:tensorflow:From /home/woody/iwbi/iwbi009h/software/privat/conda/envs/myenv/lib/python3.10/site-packages/keras/layers/normalization/batch_normalization.py:561: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "training started at 10-Feb-2023 12:58 pm\n",
      "parameters are: Epochs: 50  BS: 1 nSteps: 18000 nSamples: 360\n",
      "Model meta graph saved in::savedModels/10Feb_1223pm_5L_1K_50E_AG/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 56/18000 [05:38<30:09:10,  6.05s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(nSteps)):\n\u001b[1;32m     71\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m         tmp,_,_\u001b[39m=\u001b[39msess\u001b[39m.\u001b[39;49mrun([loss,update_ops,opToRun])\n\u001b[1;32m     73\u001b[0m         totalLoss\u001b[39m.\u001b[39mappend(tmp)\n\u001b[1;32m     74\u001b[0m         \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mremainder(step\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m,nBatch)\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/home/woody/iwbi/iwbi009h/software/privat/conda/envs/myenv/lib/python3.10/site-packages/tensorflow/python/client/session.py:968\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    965\u001b[0m run_metadata_ptr \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_NewBuffer() \u001b[39mif\u001b[39;00m run_metadata \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    967\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(\u001b[39mNone\u001b[39;49;00m, fetches, feed_dict, options_ptr,\n\u001b[1;32m    969\u001b[0m                      run_metadata_ptr)\n\u001b[1;32m    970\u001b[0m   \u001b[39mif\u001b[39;00m run_metadata:\n\u001b[1;32m    971\u001b[0m     proto_data \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[0;32m/home/woody/iwbi/iwbi009h/software/privat/conda/envs/myenv/lib/python3.10/site-packages/tensorflow/python/client/session.py:1191\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[39m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m \u001b[39m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \u001b[39mif\u001b[39;00m final_fetches \u001b[39mor\u001b[39;00m final_targets \u001b[39mor\u001b[39;00m (handle \u001b[39mand\u001b[39;00m feed_dict_tensor):\n\u001b[0;32m-> 1191\u001b[0m   results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_run(handle, final_targets, final_fetches,\n\u001b[1;32m   1192\u001b[0m                          feed_dict_tensor, options, run_metadata)\n\u001b[1;32m   1193\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1194\u001b[0m   results \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/home/woody/iwbi/iwbi009h/software/privat/conda/envs/myenv/lib/python3.10/site-packages/tensorflow/python/client/session.py:1371\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1368\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[1;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m handle \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1371\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m   1372\u001b[0m                        run_metadata)\n\u001b[1;32m   1373\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[0;32m/home/woody/iwbi/iwbi009h/software/privat/conda/envs/myenv/lib/python3.10/site-packages/tensorflow/python/client/session.py:1378\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_do_call\u001b[39m(\u001b[39mself\u001b[39m, fn, \u001b[39m*\u001b[39margs):\n\u001b[1;32m   1377\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m   1379\u001b[0m   \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mOpError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1380\u001b[0m     message \u001b[39m=\u001b[39m compat\u001b[39m.\u001b[39mas_text(e\u001b[39m.\u001b[39mmessage)\n",
      "File \u001b[0;32m/home/woody/iwbi/iwbi009h/software/privat/conda/envs/myenv/lib/python3.10/site-packages/tensorflow/python/client/session.py:1361\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[1;32m   1359\u001b[0m   \u001b[39m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[1;32m   1360\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extend_graph()\n\u001b[0;32m-> 1361\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m   1362\u001b[0m                                   target_list, run_metadata)\n",
      "File \u001b[0;32m/home/woody/iwbi/iwbi009h/software/privat/conda/envs/myenv/lib/python3.10/site-packages/tensorflow/python/client/session.py:1454\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1452\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_tf_sessionrun\u001b[39m(\u001b[39mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[1;32m   1453\u001b[0m                         run_metadata):\n\u001b[0;32m-> 1454\u001b[0m   \u001b[39mreturn\u001b[39;00m tf_session\u001b[39m.\u001b[39;49mTF_SessionRun_wrapper(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_session, options, feed_dict,\n\u001b[1;32m   1455\u001b[0m                                           fetch_list, target_list,\n\u001b[1;32m   1456\u001b[0m                                           run_metadata)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "saver=tf.compat.v1.train.Saver()\n",
    "with tf.compat.v1.Session(config=config) as sess:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    savedFile=saver.save(sess, sessFileNameTst,latest_filename='checkpointTst')\n",
    "print ('testing model saved:' +savedFile)\n",
    "#%% read multi-channel dataset\n",
    "trnOrg,trnAtb,trnCsm,trnMask=sf.getData('training')\n",
    "trnOrg,trnAtb=sf.c2r(trnOrg),sf.c2r(trnAtb)\n",
    "\n",
    "#%%\n",
    "tf.compat.v1.reset_default_graph()\n",
    "csmP = tf.compat.v1.placeholder(tf.complex64,shape=(None,None,None,None),name='csm')\n",
    "maskP= tf.compat.v1.placeholder(tf.complex64,shape=(None,None,None),name='mask')\n",
    "atbP = tf.compat.v1.placeholder(tf.float32,shape=(None,None,None,2),name='atb')\n",
    "orgP = tf.compat.v1.placeholder(tf.float32,shape=(None,None,None,2),name='org')\n",
    "\n",
    "\n",
    "#%% creating the dataset\n",
    "nTrn=trnOrg.shape[0]\n",
    "nBatch= int(np.floor(np.float32(nTrn)/batchSize))\n",
    "nSteps= nBatch*epochs\n",
    "\n",
    "trnData = tf.data.Dataset.from_tensor_slices((orgP,atbP,csmP,maskP))\n",
    "trnData = trnData.cache()\n",
    "trnData=trnData.repeat(count=epochs)\n",
    "trnData = trnData.shuffle(buffer_size=trnOrg.shape[0])\n",
    "trnData=trnData.batch(batchSize)\n",
    "trnData=trnData.prefetch(5)\n",
    "iterator=tf.compat.v1.data.make_initializable_iterator(trnData)\n",
    "orgT,atbT,csmT,maskT = iterator.get_next('getNext')\n",
    "\n",
    "#%% make training model\n",
    "\n",
    "out=mm.makeModel(atbT,csmT,maskT,True,nLayers,K,gradientMethod)\n",
    "predT=out['dc'+str(K)]\n",
    "predT=tf.identity(predT,name='pred')\n",
    "loss = tf.reduce_mean(input_tensor=tf.reduce_sum(input_tensor=tf.pow(predT-orgT, 2),axis=0))\n",
    "tf.compat.v1.summary.scalar('loss', loss)\n",
    "update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.compat.v1.name_scope('optimizer'):\n",
    "    optimizer = tf.compat.v1.train.AdamOptimizer()\n",
    "    gvs = optimizer.compute_gradients(loss)\n",
    "    capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "    opToRun=optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "\n",
    "#%% training code\n",
    "\n",
    "\n",
    "print ('training started at', datetime.now().strftime(\"%d-%b-%Y %I:%M %P\"))\n",
    "print ('parameters are: Epochs:',epochs,' BS:',batchSize,'nSteps:',nSteps,'nSamples:',nTrn)\n",
    "\n",
    "saver = tf.compat.v1.train.Saver(max_to_keep=100)\n",
    "totalLoss,ep=[],0\n",
    "lossT = tf.compat.v1.placeholder(tf.float32)\n",
    "lossSumT = tf.compat.v1.summary.scalar(\"TrnLoss\", lossT)\n",
    "\n",
    "with tf.compat.v1.Session(config=config) as sess:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    if restoreWeights:\n",
    "        sess=sf.assignWts(sess,nLayers,wts)\n",
    "\n",
    "    feedDict={orgP:trnOrg,atbP:trnAtb, maskP:trnMask,csmP:trnCsm}\n",
    "    sess.run(iterator.initializer,feed_dict=feedDict)\n",
    "    savedFile=saver.save(sess, sessFileName)\n",
    "    print(\"Model meta graph saved in::%s\" % savedFile)\n",
    "\n",
    "    writer = tf.compat.v1.summary.FileWriter(directory, sess.graph)\n",
    "    for step in tqdm(range(nSteps)):\n",
    "        try:\n",
    "            tmp,_,_=sess.run([loss,update_ops,opToRun])\n",
    "            totalLoss.append(tmp)\n",
    "            if np.remainder(step+1,nBatch)==0:\n",
    "                ep=ep+1\n",
    "                avgTrnLoss=np.mean(totalLoss)\n",
    "                lossSum=sess.run(lossSumT,feed_dict={lossT:avgTrnLoss})\n",
    "                writer.add_summary(lossSum,ep)\n",
    "                totalLoss=[] #after each epoch empty the list of total loos\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "    savedfile=saver.save(sess, sessFileName,global_step=ep,write_meta_graph=True)\n",
    "    writer.close()\n",
    "\n",
    "end_time = time.time()\n",
    "print ('Trianing completed in minutes ', ((end_time - start_time) / 60))\n",
    "print ('training completed at', datetime.now().strftime(\"%d-%b-%Y %I:%M %P\"))\n",
    "print ('*************************************************')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "6d4bcad7c14e08c831763fb13dfafb1ceb9163893c011df81118bdf0c9ab3c86"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
